package org.apache.kafka.clients.consumer;
import org.apache.kafka.common.header.Headers;
import org.apache.kafka.common.header.internals.RecordHeaders;
import org.apache.kafka.common.record.DefaultRecord;
import org.apache.kafka.common.record.RecordBatch;
import org.apache.kafka.common.record.TimestampType;
import java.util.Optional;
public class ConsumerRecord<K,V> {
  public static final long NO_TIMESTAMP=RecordBatch.NO_TIMESTAMP;
  public static final int NULL_SIZE=-1;
  public static final int NULL_CHECKSUM=-1;
  private final String topic;
  private final int partition;
  private final long offset;
  private final long timestamp;
  private final TimestampType timestampType;
  private final int serializedKeySize;
  private final int serializedValueSize;
  private final Headers headers;
  private final K key;
  private final V value;
  private final Optional<Integer> leaderEpoch;
  private volatile Long checksum;
  public ConsumerRecord(  String topic,  int partition,  long offset,  K key,  V value){
    this(topic,partition,offset,NO_TIMESTAMP,TimestampType.NO_TIMESTAMP_TYPE,NULL_CHECKSUM,NULL_SIZE,NULL_SIZE,key,value);
  }
  public ConsumerRecord(  String topic,  int partition,  long offset,  long timestamp,  TimestampType timestampType,  long checksum,  int serializedKeySize,  int serializedValueSize,  K key,  V value){
    this(topic,partition,offset,timestamp,timestampType,checksum,serializedKeySize,serializedValueSize,key,value,new RecordHeaders());
  }
  public ConsumerRecord(  String topic,  int partition,  long offset,  long timestamp,  TimestampType timestampType,  Long checksum,  int serializedKeySize,  int serializedValueSize,  K key,  V value,  Headers headers){
    this(topic,partition,offset,timestamp,timestampType,checksum,serializedKeySize,serializedValueSize,key,value,headers,Optional.empty());
  }
  public ConsumerRecord(  String topic,  int partition,  long offset,  long timestamp,  TimestampType timestampType,  Long checksum,  int serializedKeySize,  int serializedValueSize,  K key,  V value,  Headers headers,  Optional<Integer> leaderEpoch){
    if (topic == null)     throw new IllegalArgumentException("Topic cannot be null");
    this.topic=topic;
    this.partition=partition;
    this.offset=offset;
    this.timestamp=timestamp;
    this.timestampType=timestampType;
    this.checksum=checksum;
    this.serializedKeySize=serializedKeySize;
    this.serializedValueSize=serializedValueSize;
    this.key=key;
    this.value=value;
    this.headers=headers;
    this.leaderEpoch=leaderEpoch;
  }
  public String topic(){
    return this.topic;
  }
  public int partition(){
    return this.partition;
  }
  public Headers headers(){
    return headers;
  }
  public K key(){
    return key;
  }
  public V value(){
    return value;
  }
  public long offset(){
    return offset;
  }
  public long timestamp(){
    return timestamp;
  }
  public TimestampType timestampType(){
    return timestampType;
  }
  @Deprecated public long checksum(){
    if (checksum == null)     this.checksum=DefaultRecord.computePartialChecksum(timestamp,serializedKeySize,serializedValueSize);
    return this.checksum;
  }
  public int serializedKeySize(){
    return this.serializedKeySize;
  }
  public int serializedValueSize(){
    return this.serializedValueSize;
  }
  public Optional<Integer> leaderEpoch(){
    return leaderEpoch;
  }
  @Override public String toString(){
    return "ConsumerRecord(topic = " + topic + ", partition = "+ partition+ ", leaderEpoch = "+ leaderEpoch.orElse(null)+ ", offset = "+ offset+ ", "+ timestampType+ " = "+ timestamp+ ", serialized key size = "+ serializedKeySize+ ", serialized value size = "+ serializedValueSize+ ", headers = "+ headers+ ", key = "+ key+ ", value = "+ value+ ")";
  }
}
