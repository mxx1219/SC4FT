package org.apache.hadoop.hdfs;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InterruptedIOException;
import java.nio.channels.ClosedChannelException;
import java.util.EnumSet;
import java.util.concurrent.atomic.AtomicReference;
import org.apache.hadoop.HadoopIllegalArgumentException;
import org.apache.hadoop.classification.InterfaceAudience;
import org.apache.hadoop.crypto.CryptoProtocolVersion;
import org.apache.hadoop.fs.CanSetDropBehind;
import org.apache.hadoop.fs.CreateFlag;
import org.apache.hadoop.fs.FSOutputSummer;
import org.apache.hadoop.fs.FileAlreadyExistsException;
import org.apache.hadoop.fs.FileEncryptionInfo;
import org.apache.hadoop.fs.ParentNotDirectoryException;
import org.apache.hadoop.fs.Syncable;
import org.apache.hadoop.fs.permission.FsPermission;
import org.apache.hadoop.hdfs.client.HdfsDataOutputStream;
import org.apache.hadoop.hdfs.client.HdfsDataOutputStream.SyncFlag;
import org.apache.hadoop.hdfs.client.impl.DfsClientConf;
import org.apache.hadoop.hdfs.protocol.DSQuotaExceededException;
import org.apache.hadoop.hdfs.protocol.DatanodeInfo;
import org.apache.hadoop.hdfs.protocol.ExtendedBlock;
import org.apache.hadoop.hdfs.protocol.HdfsFileStatus;
import org.apache.hadoop.hdfs.protocol.LocatedBlock;
import org.apache.hadoop.hdfs.protocol.NSQuotaExceededException;
import org.apache.hadoop.hdfs.protocol.QuotaByStorageTypeExceededException;
import org.apache.hadoop.hdfs.protocol.SnapshotAccessControlException;
import org.apache.hadoop.hdfs.protocol.UnresolvedPathException;
import org.apache.hadoop.hdfs.protocol.datatransfer.PacketHeader;
import org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier;
import org.apache.hadoop.hdfs.server.datanode.CachingStrategy;
import org.apache.hadoop.hdfs.server.namenode.RetryStartFileException;
import org.apache.hadoop.hdfs.server.namenode.SafeModeException;
import org.apache.hadoop.hdfs.util.ByteArrayManager;
import org.apache.hadoop.io.EnumSetWritable;
import org.apache.hadoop.ipc.RemoteException;
import org.apache.hadoop.security.AccessControlException;
import org.apache.hadoop.security.token.Token;
import org.apache.hadoop.util.DataChecksum;
import org.apache.hadoop.util.DataChecksum.Type;
import org.apache.hadoop.util.Progressable;
import org.apache.hadoop.util.Time;
import org.apache.htrace.Sampler;
import org.apache.htrace.Trace;
import org.apache.htrace.TraceScope;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.base.Preconditions;
@InterfaceAudience.Private public class DFSOutputStream extends FSOutputSummer implements Syncable, CanSetDropBehind {
  @VisibleForTesting static final int CREATE_RETRY_COUNT=10;
  @VisibleForTesting static CryptoProtocolVersion[] SUPPORTED_CRYPTO_VERSIONS=CryptoProtocolVersion.supported();
  protected final DFSClient dfsClient;
  protected final ByteArrayManager byteArrayManager;
  protected volatile boolean closed=false;
  protected final String src;
  protected final long fileId;
  protected final long blockSize;
  protected final int bytesPerChecksum;
  protected DFSPacket currentPacket=null;
  protected DataStreamer streamer;
  protected int packetSize=0;
  protected int chunksPerPacket=0;
  protected long lastFlushOffset=0;
  private long initialFileSize=0;
  private final short blockReplication;
  protected boolean shouldSyncBlock=false;
  protected final AtomicReference<CachingStrategy> cachingStrategy;
  private FileEncryptionInfo fileEncryptionInfo;
  protected DFSPacket createPacket(  int packetSize,  int chunksPerPkt,  long offsetInBlock,  long seqno,  boolean lastPacketInBlock) throws InterruptedIOException {
    final byte[] buf;
    final int bufferSize=PacketHeader.PKT_MAX_HEADER_LEN + packetSize;
    try {
      buf=byteArrayManager.newByteArray(bufferSize);
    }
 catch (    InterruptedException ie) {
      final InterruptedIOException iioe=new InterruptedIOException("seqno=" + seqno);
      iioe.initCause(ie);
      throw iioe;
    }
    return new DFSPacket(buf,chunksPerPkt,offsetInBlock,seqno,getChecksumSize(),lastPacketInBlock);
  }
  @Override protected void checkClosed() throws IOException {
    if (isClosed()) {
      streamer.getLastException().throwException4Close();
    }
  }
  @VisibleForTesting public synchronized DatanodeInfo[] getPipeline(){
    if (streamer.streamerClosed()) {
      return null;
    }
    DatanodeInfo[] currentNodes=streamer.getNodes();
    if (currentNodes == null) {
      return null;
    }
    DatanodeInfo[] value=new DatanodeInfo[currentNodes.length];
    for (int i=0; i < currentNodes.length; i++) {
      value[i]=currentNodes[i];
    }
    return value;
  }
  private static DataChecksum getChecksum4Compute(  DataChecksum checksum,  HdfsFileStatus stat){
    if (DataStreamer.isLazyPersist(stat) && stat.getReplication() == 1) {
      return DataChecksum.newDataChecksum(Type.NULL,checksum.getBytesPerChecksum());
    }
    return checksum;
  }
  private DFSOutputStream(  DFSClient dfsClient,  String src,  Progressable progress,  HdfsFileStatus stat,  DataChecksum checksum) throws IOException {
    super(getChecksum4Compute(checksum,stat));
    this.dfsClient=dfsClient;
    this.src=src;
    this.fileId=stat.getFileId();
    this.blockSize=stat.getBlockSize();
    this.blockReplication=stat.getReplication();
    this.fileEncryptionInfo=stat.getFileEncryptionInfo();
    this.cachingStrategy=new AtomicReference<CachingStrategy>(dfsClient.getDefaultWriteCachingStrategy());
    if ((progress != null) && DFSClient.LOG.isDebugEnabled()) {
      DFSClient.LOG.debug("Set non-null progress callback on DFSOutputStream " + src);
    }
    this.bytesPerChecksum=checksum.getBytesPerChecksum();
    if (bytesPerChecksum <= 0) {
      throw new HadoopIllegalArgumentException("Invalid value: bytesPerChecksum = " + bytesPerChecksum + " <= 0");
    }
    if (blockSize % bytesPerChecksum != 0) {
      throw new HadoopIllegalArgumentException("Invalid values: " + DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY + " (="+ bytesPerChecksum+ ") must divide block size (="+ blockSize+ ").");
    }
    this.byteArrayManager=dfsClient.getClientContext().getByteArrayManager();
  }
  protected DFSOutputStream(  DFSClient dfsClient,  String src,  HdfsFileStatus stat,  EnumSet<CreateFlag> flag,  Progressable progress,  DataChecksum checksum,  String[] favoredNodes) throws IOException {
    this(dfsClient,src,progress,stat,checksum);
    this.shouldSyncBlock=flag.contains(CreateFlag.SYNC_BLOCK);
    computePacketChunkSize(dfsClient.getConf().getWritePacketSize(),bytesPerChecksum);
    streamer=new DataStreamer(stat,null,dfsClient,src,progress,checksum,cachingStrategy,byteArrayManager,favoredNodes);
  }
  static DFSOutputStream newStreamForCreate(  DFSClient dfsClient,  String src,  FsPermission masked,  EnumSet<CreateFlag> flag,  boolean createParent,  short replication,  long blockSize,  Progressable progress,  int buffersize,  DataChecksum checksum,  String[] favoredNodes) throws IOException {
    TraceScope scope=dfsClient.getPathTraceScope("newStreamForCreate",src);
    try {
      HdfsFileStatus stat=null;
      boolean shouldRetry=true;
      int retryCount=CREATE_RETRY_COUNT;
      while (shouldRetry) {
        shouldRetry=false;
        try {
          stat=dfsClient.namenode.create(src,masked,dfsClient.clientName,new EnumSetWritable<CreateFlag>(flag),createParent,replication,blockSize,SUPPORTED_CRYPTO_VERSIONS);
          break;
        }
 catch (        RemoteException re) {
          IOException e=re.unwrapRemoteException(AccessControlException.class,DSQuotaExceededException.class,QuotaByStorageTypeExceededException.class,FileAlreadyExistsException.class,FileNotFoundException.class,ParentNotDirectoryException.class,NSQuotaExceededException.class,RetryStartFileException.class,SafeModeException.class,UnresolvedPathException.class,SnapshotAccessControlException.class,UnknownCryptoProtocolVersionException.class);
          if (e instanceof RetryStartFileException) {
            if (retryCount > 0) {
              shouldRetry=true;
              retryCount--;
            }
 else {
              throw new IOException("Too many retries because of encryption" + " zone operations",e);
            }
          }
 else {
            throw e;
          }
        }
      }
      Preconditions.checkNotNull(stat,"HdfsFileStatus should not be null!");
      final DFSOutputStream out=new DFSOutputStream(dfsClient,src,stat,flag,progress,checksum,favoredNodes);
      out.start();
      return out;
    }
  finally {
      scope.close();
    }
  }
  private DFSOutputStream(  DFSClient dfsClient,  String src,  EnumSet<CreateFlag> flags,  Progressable progress,  LocatedBlock lastBlock,  HdfsFileStatus stat,  DataChecksum checksum,  String[] favoredNodes) throws IOException {
    this(dfsClient,src,progress,stat,checksum);
    initialFileSize=stat.getLen();
    this.shouldSyncBlock=flags.contains(CreateFlag.SYNC_BLOCK);
    boolean toNewBlock=flags.contains(CreateFlag.NEW_BLOCK);
    this.fileEncryptionInfo=stat.getFileEncryptionInfo();
    if (!toNewBlock && lastBlock != null) {
      streamer=new DataStreamer(lastBlock,stat,dfsClient,src,progress,checksum,cachingStrategy,byteArrayManager);
      streamer.setBytesCurBlock(lastBlock.getBlockSize());
      adjustPacketChunkSize(stat);
      streamer.setPipelineInConstruction(lastBlock);
    }
 else {
      computePacketChunkSize(dfsClient.getConf().getWritePacketSize(),bytesPerChecksum);
      streamer=new DataStreamer(stat,lastBlock != null ? lastBlock.getBlock() : null,dfsClient,src,progress,checksum,cachingStrategy,byteArrayManager,favoredNodes);
    }
  }
  private void adjustPacketChunkSize(  HdfsFileStatus stat) throws IOException {
    long usedInLastBlock=stat.getLen() % blockSize;
    int freeInLastBlock=(int)(blockSize - usedInLastBlock);
    int usedInCksum=(int)(stat.getLen() % bytesPerChecksum);
    int freeInCksum=bytesPerChecksum - usedInCksum;
    if (freeInLastBlock == blockSize) {
      throw new IOException("The last block for file " + src + " is full.");
    }
    if (usedInCksum > 0 && freeInCksum > 0) {
      computePacketChunkSize(0,freeInCksum);
      setChecksumBufSize(freeInCksum);
      streamer.setAppendChunk(true);
    }
 else {
      computePacketChunkSize(Math.min(dfsClient.getConf().getWritePacketSize(),freeInLastBlock),bytesPerChecksum);
    }
  }
  static DFSOutputStream newStreamForAppend(  DFSClient dfsClient,  String src,  EnumSet<CreateFlag> flags,  int bufferSize,  Progressable progress,  LocatedBlock lastBlock,  HdfsFileStatus stat,  DataChecksum checksum,  String[] favoredNodes) throws IOException {
    TraceScope scope=dfsClient.getPathTraceScope("newStreamForAppend",src);
    try {
      final DFSOutputStream out=new DFSOutputStream(dfsClient,src,flags,progress,lastBlock,stat,checksum,favoredNodes);
      out.start();
      return out;
    }
  finally {
      scope.close();
    }
  }
  protected void computePacketChunkSize(  int psize,  int csize){
    final int bodySize=psize - PacketHeader.PKT_MAX_HEADER_LEN;
    final int chunkSize=csize + getChecksumSize();
    chunksPerPacket=Math.max(bodySize / chunkSize,1);
    packetSize=chunkSize * chunksPerPacket;
    if (DFSClient.LOG.isDebugEnabled()) {
      DFSClient.LOG.debug("computePacketChunkSize: src=" + src + ", chunkSize="+ chunkSize+ ", chunksPerPacket="+ chunksPerPacket+ ", packetSize="+ packetSize);
    }
  }
  protected TraceScope createWriteTraceScope(){
    return dfsClient.getPathTraceScope("DFSOutputStream#write",src);
  }
  @Override protected synchronized void writeChunk(  byte[] b,  int offset,  int len,  byte[] checksum,  int ckoff,  int cklen) throws IOException {
    dfsClient.checkOpen();
    checkClosed();
    if (len > bytesPerChecksum) {
      throw new IOException("writeChunk() buffer size is " + len + " is larger than supported  bytesPerChecksum "+ bytesPerChecksum);
    }
    if (cklen != 0 && cklen != getChecksumSize()) {
      throw new IOException("writeChunk() checksum size is supposed to be " + getChecksumSize() + " but found to be "+ cklen);
    }
    if (currentPacket == null) {
      currentPacket=createPacket(packetSize,chunksPerPacket,streamer.getBytesCurBlock(),streamer.getAndIncCurrentSeqno(),false);
      if (DFSClient.LOG.isDebugEnabled()) {
        DFSClient.LOG.debug("DFSClient writeChunk allocating new packet seqno=" + currentPacket.getSeqno() + ", src="+ src+ ", packetSize="+ packetSize+ ", chunksPerPacket="+ chunksPerPacket+ ", bytesCurBlock="+ streamer.getBytesCurBlock());
      }
    }
    currentPacket.writeChecksum(checksum,ckoff,cklen);
    currentPacket.writeData(b,offset,len);
    currentPacket.incNumChunks();
    streamer.incBytesCurBlock(len);
    if (currentPacket.getNumChunks() == currentPacket.getMaxChunks() || streamer.getBytesCurBlock() == blockSize) {
      if (DFSClient.LOG.isDebugEnabled()) {
        DFSClient.LOG.debug("DFSClient writeChunk packet full seqno=" + currentPacket.getSeqno() + ", src="+ src+ ", bytesCurBlock="+ streamer.getBytesCurBlock()+ ", blockSize="+ blockSize+ ", appendChunk="+ streamer.getAppendChunk());
      }
      streamer.waitAndQueuePacket(currentPacket);
      currentPacket=null;
      adjustChunkBoundary();
      endBlock();
    }
  }
  protected void adjustChunkBoundary(){
    if (streamer.getAppendChunk() && streamer.getBytesCurBlock() % bytesPerChecksum == 0) {
      streamer.setAppendChunk(false);
      resetChecksumBufSize();
    }
    if (!streamer.getAppendChunk()) {
      int psize=Math.min((int)(blockSize - streamer.getBytesCurBlock()),dfsClient.getConf().getWritePacketSize());
      computePacketChunkSize(psize,bytesPerChecksum);
    }
  }
  protected void endBlock() throws IOException {
    if (streamer.getBytesCurBlock() == blockSize) {
      currentPacket=createPacket(0,0,streamer.getBytesCurBlock(),streamer.getAndIncCurrentSeqno(),true);
      currentPacket.setSyncBlock(shouldSyncBlock);
      streamer.waitAndQueuePacket(currentPacket);
      currentPacket=null;
      streamer.setBytesCurBlock(0);
      lastFlushOffset=0;
    }
  }
  @Deprecated public void sync() throws IOException {
    hflush();
  }
  @Override public void hflush() throws IOException {
    TraceScope scope=dfsClient.getPathTraceScope("hflush",src);
    try {
      flushOrSync(false,EnumSet.noneOf(SyncFlag.class));
    }
  finally {
      scope.close();
    }
  }
  @Override public void hsync() throws IOException {
    TraceScope scope=dfsClient.getPathTraceScope("hsync",src);
    try {
      flushOrSync(true,EnumSet.noneOf(SyncFlag.class));
    }
  finally {
      scope.close();
    }
  }
  public void hsync(  EnumSet<SyncFlag> syncFlags) throws IOException {
    TraceScope scope=dfsClient.getPathTraceScope("hsync",src);
    try {
      flushOrSync(true,syncFlags);
    }
  finally {
      scope.close();
    }
  }
  private void flushOrSync(  boolean isSync,  EnumSet<SyncFlag> syncFlags) throws IOException {
  }
  @Deprecated public synchronized int getNumCurrentReplicas() throws IOException {
    return getCurrentBlockReplication();
  }
  public synchronized int getCurrentBlockReplication() throws IOException {
    dfsClient.checkOpen();
    checkClosed();
    if (streamer.streamerClosed()) {
      return blockReplication;
    }
    DatanodeInfo[] currentNodes=streamer.getNodes();
    if (currentNodes == null) {
      return blockReplication;
    }
    return currentNodes.length;
  }
  protected void flushInternal() throws IOException {
    long toWaitFor;
synchronized (this) {
      dfsClient.checkOpen();
      checkClosed();
      streamer.queuePacket(currentPacket);
      currentPacket=null;
      toWaitFor=streamer.getLastQueuedSeqno();
    }
    streamer.waitForAckedSeqno(toWaitFor);
  }
  protected synchronized void start(){
    streamer.start();
  }
  synchronized void abort() throws IOException {
    if (isClosed()) {
      return;
    }
    streamer.getLastException().set(new IOException("Lease timeout of " + (dfsClient.getConf().getHdfsTimeout() / 1000) + " seconds expired."));
    closeThreads(true);
    dfsClient.endFileLease(fileId);
  }
  boolean isClosed(){
    return closed || streamer.streamerClosed();
  }
  void setClosed(){
    closed=true;
    streamer.release();
  }
  protected void closeThreads(  boolean force) throws IOException {
    try {
      streamer.close(force);
      streamer.join();
      streamer.closeSocket();
    }
 catch (    InterruptedException e) {
      throw new IOException("Failed to shutdown streamer");
    }
 finally {
      streamer.setSocketToNull();
      setClosed();
    }
  }
  @Override public synchronized void close() throws IOException {
    TraceScope scope=dfsClient.getPathTraceScope("DFSOutputStream#close",src);
    try {
      closeImpl();
    }
  finally {
      scope.close();
    }
  }
  protected synchronized void closeImpl() throws IOException {
    if (isClosed()) {
      streamer.getLastException().check();
      return;
    }
    try {
      flushBuffer();
      if (currentPacket != null) {
        streamer.waitAndQueuePacket(currentPacket);
        currentPacket=null;
      }
      if (streamer.getBytesCurBlock() != 0) {
        currentPacket=createPacket(0,0,streamer.getBytesCurBlock(),streamer.getAndIncCurrentSeqno(),true);
        currentPacket.setSyncBlock(shouldSyncBlock);
      }
      flushInternal();
      ExtendedBlock lastBlock=streamer.getBlock();
      closeThreads(false);
      TraceScope scope=Trace.startSpan("completeFile",Sampler.NEVER);
      try {
        completeFile(lastBlock);
      }
  finally {
        scope.close();
      }
      dfsClient.endFileLease(fileId);
    }
 catch (    ClosedChannelException e) {
    }
 finally {
      setClosed();
    }
  }
  protected void completeFile(  ExtendedBlock last) throws IOException {
    long localstart=Time.monotonicNow();
    final DfsClientConf conf=dfsClient.getConf();
    long sleeptime=conf.getBlockWriteLocateFollowingInitialDelayMs();
    boolean fileComplete=false;
    int retries=conf.getNumBlockWriteLocateFollowingRetry();
    while (!fileComplete) {
      fileComplete=dfsClient.namenode.complete(src,dfsClient.clientName,last,fileId);
      if (!fileComplete) {
        final int hdfsTimeout=conf.getHdfsTimeout();
        if (!dfsClient.clientRunning || (hdfsTimeout > 0 && localstart + hdfsTimeout < Time.monotonicNow())) {
          String msg="Unable to close file because dfsclient " + " was unable to contact the HDFS servers." + " clientRunning " + dfsClient.clientRunning + " hdfsTimeout "+ hdfsTimeout;
          DFSClient.LOG.info(msg);
          throw new IOException(msg);
        }
        try {
          if (retries == 0) {
            throw new IOException("Unable to close file because the last block" + " does not have enough number of replicas.");
          }
          retries--;
          Thread.sleep(sleeptime);
          sleeptime*=2;
          if (Time.monotonicNow() - localstart > 5000) {
            DFSClient.LOG.info("Could not complete " + src + " retrying...");
          }
        }
 catch (        InterruptedException ie) {
          DFSClient.LOG.warn("Caught exception ",ie);
        }
      }
    }
  }
  @VisibleForTesting public void setArtificialSlowdown(  long period){
    streamer.setArtificialSlowdown(period);
  }
  @VisibleForTesting public synchronized void setChunksPerPacket(  int value){
    chunksPerPacket=Math.min(chunksPerPacket,value);
    packetSize=(bytesPerChecksum + getChecksumSize()) * chunksPerPacket;
  }
  public long getInitialLen(){
    return initialFileSize;
  }
  public FileEncryptionInfo getFileEncryptionInfo(){
    return fileEncryptionInfo;
  }
  synchronized Token<BlockTokenIdentifier> getBlockToken(){
    return streamer.getBlockToken();
  }
  @Override public void setDropBehind(  Boolean dropBehind) throws IOException {
    CachingStrategy prevStrategy, nextStrategy;
    do {
      prevStrategy=this.cachingStrategy.get();
      nextStrategy=new CachingStrategy.Builder(prevStrategy).setDropBehind(dropBehind).build();
    }
 while (!this.cachingStrategy.compareAndSet(prevStrategy,nextStrategy));
  }
  @VisibleForTesting ExtendedBlock getBlock(){
    return streamer.getBlock();
  }
  @VisibleForTesting public long getFileId(){
    return fileId;
  }
}
